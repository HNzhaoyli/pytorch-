{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch知识储备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch 是美国互联网巨头Facebook 在深度学习框架torch 的基础上使用Python重写的一个全新的深度学习框架，更像Numpy 的替代产物，继承了Numpy 的众多优点，还支持GPUs计算和自动梯度求导等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、pytorch中的Tensor，   Tensor在PyTorch 中负责存储基本数据，PyTorch针对Tensor也提供了相对丰富的函数和方法，所以PyTorch中的Tensor与NumPy的数组具有极高的相似性。Tensor是一种高层次架构，也不要明白什么是深度学习，什么是后向传播，如何对模型进行优化，什么是计算图等技术细节。更重要的是，在PyTorch中定义的Tensor数据类型可以在GPUs上进行运算，而且只需要对变量做一些简单的类型转换就能轻易实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在使用Tensor时，我们首先要掌握如何使用Tensor来定义不同数据类型的变量。和Numpy差不多，PyTorch中的Tensor也有自己的数据类型定义方式，常用的如下：\n",
    "\n",
    "1.1.1 torch.FloatTensor\n",
    "　　此变量用于生成数据类型为浮点型的Tensor，传递给torch.FloatTensor的参数可以是一个列表，也可以是一个维度值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import  torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.FloatTensor(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.arange(4).reshape(2,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3., 4., 5.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.FloatTensor([2,3,4,5])\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可以看到，打印输出的两组变量数据类型都显示为浮点型，不同的是，前一组的是按照我们指定的维度随机生成的浮点型Tensor，而另外一组是按照我们给定的列表生成的浮点型Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 torch.IntTensor\n",
    "　　用于生成数据类型为整型的Tensor，传递给torch.IntTensor的参数可以是一个列表，也可以是一个维度值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  torch\n",
    "a = torch.IntTensor(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 torch.rand\n",
    "　　用于生成数据类型为浮点型且维度指定的随机Tensor，和在Numpy中使用numpy.rand生成随机数的方法类似，随机生成的浮点数据在0~1区间均匀分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3404, 0.6275, 0.6320],\n",
       "        [0.1833, 0.1735, 0.4264]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2,3)# 0-1区间均匀分布\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 torch.randn\n",
    "　　用于生成数据类型为浮点型且维度指定的随机Tensor，和在Numpy中使用numpy.randn生成随机数的方法类似，随机生成的浮点数的取值满足均值为0，方差为1的正太分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3870,  0.4331, -0.8726],\n",
       "        [-0.0213,  0.9214,  1.4082]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2,3)#均值为0，方差为1 的正太分布\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 torch.range\n",
    "　　用于生成数据类型为浮点型且自定义其实范围和结束范围的Tensor，所以传递给torch.range的参数有三个，分别是范围的起始值，范围的结束值和步长，其中，步长用于指定从起始值到结束值的每步的数据间隔。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6., 8.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.range(2,8,2)#(start,end,step)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.6 torch.zeros\n",
    "　　用于生成数据类型为浮点型且维度指定的Tensor，不过这个浮点型的Tensor中的元素值全部为0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Tensor的运算\n",
    "　　这里通常对Tensor数据类型的变量进行运算，来组合一些简单或者复杂的算法，常用的Tensor运算如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 torch.abs\n",
    "　　　将参数传递到torch.abs后返回输入参数的绝对值作为输出，输出参数必须是一个Tensor数据类型的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1951, -0.5282, -0.0121],\n",
      "        [ 0.3914,  0.5650,  0.8784]])\n",
      "tensor([[0.1951, 0.5282, 0.0121],\n",
      "        [0.3914, 0.5650, 0.8784]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "print(a)\n",
    "b = torch.abs(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 torch.add\n",
    "　　将参数传递到torch.add后返回输入参数的求和结果作为输出，输入参数既可以全部是Tensor数据类型的变量，也可以是一个Tensor数据类型的变量，另一个是标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6309,  0.7126, -0.6616],\n",
      "        [-0.2515,  0.6435,  0.5660]])\n",
      "tensor([[-1.5737, -1.0938,  0.4766],\n",
      "        [ 0.7527,  1.5362, -0.4424]])\n",
      "tensor([[-0.9427, -0.3812, -0.1850],\n",
      "        [ 0.5012,  2.1797,  0.1236]])\n",
      "tensor([[10.6309, 10.7126,  9.3384],\n",
      "        [ 9.7485, 10.6435, 10.5660]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "print(a)\n",
    "b = torch.randn(2,3)\n",
    "print(b)\n",
    "c = torch.add(a,b)\n",
    "print(c)\n",
    "d = torch.add(a,10)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 减法   sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3., 4., 5., 6.]) \n",
      " tensor([1., 2., 3., 4., 5.])\n",
      "****************** \n",
      " tensor([1., 1., 1., 1., 1.]) \n",
      " tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.range(2,6)\n",
    "b = torch.range(1,5)\n",
    "print(a,\"\\n\",b)\n",
    "c = a-b\n",
    "d = torch.sub(a,b)\n",
    "print(\"******************\",\"\\n\",c,\"\\n\",d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 如上所示，无论是调用torch.add对两个Tensor数据类型的变量进行计算，还是完成Tensor数据类型的变量和标量的计算，计算方式都和NumPy中的数组的加法运算如出一辙。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 torch.clamp\n",
    "　　对输入参数按照自定义的范围进行裁剪，最后将参数裁剪的结果作为输出。所以输入参数一共有三个，分别是需要进行裁剪的Tensor数据类型的变量、裁剪的上边界和裁剪的下边界，具体的裁剪过程是：使用变量中的每个元素分别和裁剪的上边界及裁剪的下边界的值进行比较，如果元素的值小于裁剪的下边界的值，该元素就被重写成裁剪的下边界的值；同理，如果元素的值大于裁剪的上边界的值，该元素就被重写成裁剪的上边界的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2669,  1.5306, -0.0642],\n",
       "        [-1.9873,  0.7120,  1.6157]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  torch \n",
    "a = torch.randn(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1000,  0.1000, -0.0642],\n",
       "        [-0.1000,  0.1000,  0.1000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.clamp(a,-0.1,0.1)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 torch.div\n",
    "　　将参数传递到torch.div后返回输入参数的求商结果作为输出，同样，参与运算的参数可以全部是Tensor数据类型的变量，也可以是Tensor数据类型的变量和标量的组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9485,  0.0311, -0.3609],\n",
      "        [-1.0914,  0.4387, -0.5218]]) \n",
      " tensor([[ 3.1141,  1.2130,  1.4227],\n",
      "        [ 0.9497,  1.5071, -0.3905]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.randn(2,3)\n",
    "b = torch.randn((2,3))\n",
    "print(a,\"\\n\",b,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6257,  0.0257, -0.2537],\n",
       "        [-1.1492,  0.2911,  1.3361]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.div(a,b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1948,  0.0031, -0.0361],\n",
       "        [-0.1091,  0.0439, -0.0522]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.div(a,10)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 torch.mul   对应元素相乘  （又名哈达玛积）\n",
    "　　将参数传递到 torch.mul后返回输入参数求积的结果作为输出，参与运算的参数可以全部是Tensor数据类型的变量，也可以是Tensor数据类型的变量和标量的组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6], dtype=torch.int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.IntTensor([1,2,3,4,5,6])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4, 5], dtype=torch.int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.IntTensor([2,3,4,5])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30, 40, 50, 60], dtype=torch.int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.mul(a,10)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10,  40,  90, 160, 250, 360], dtype=torch.int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.mul(a,c)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 torch.pow\n",
    "　　将参数传递到torch.pow后返回输入参数的求幂结果作为输出，参与运算的参数可以全部是Tensor数据类型的变量，也可以是Tensor数据类型的变量和标量的组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8510, -0.5559,  2.2812],\n",
      "        [-0.4742, -1.1296,  1.5330]])\n",
      "tensor([[0.7242, 0.3091, 5.2038],\n",
      "        [0.2248, 1.2760, 2.3501]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "print(a)\n",
    " \n",
    "b = torch.pow(a,2)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.7 torch.mm\n",
    "　　将参数传递到 torch.mm后返回输入参数的求积结果作为输出，不过这个求积的方式和之前的torch.mul运算方式不太样，torch.mm运用矩阵之间的乘法规则进行计算，所以被传入的参数会被当作矩阵进行处理，参数的维度自然也要满足矩阵乘法的前提条件，即前一个矩阵的行数必须和后一个矩阵的列数相等，否则不能进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9074,  2.9904,  1.6371],\n",
      "        [ 1.4120, -1.1595,  0.0382]])\n",
      "tensor([[ 0.3414,  0.3819],\n",
      "        [-0.0671,  0.2049],\n",
      "        [-0.6699, -0.3864]])\n",
      "tensor([[-1.6071, -0.3662],\n",
      "        [ 0.5342,  0.2868]])\n"
     ]
    }
   ],
   "source": [
    " \n",
    "a = torch.randn(2,3)\n",
    "print(a)\n",
    " \n",
    "b = torch.randn(3,2)\n",
    "print(b)\n",
    " \n",
    "b = torch.mm(a,b)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.8 torch.mv\n",
    "　　 将参数传递到torch.mv后返回输入参数的求积结果作为输出，torch.mv运用矩阵与向量之间的乘法规则进行计算，被传入的参数中的第1个参数代表矩阵，第2个参数代表向量，顺序不能颠倒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3260,  0.6901, -1.8888],\n",
      "        [ 2.1988, -2.4703, -1.6875]]) torch.Size([2, 3])\n",
      "tensor([ 0.3825, -0.3059,  0.9956]) torch.Size([3])\n",
      "tensor([-1.9670, -0.0833]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    " \n",
    "a = torch.randn(2,3)\n",
    "print(a,a.shape)\n",
    " \n",
    "b = torch.randn(3)\n",
    "print(b,b.shape)\n",
    " \n",
    "c = torch.mv(a,b)\n",
    "print(c,c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "range() missing 1 required positional arguments: \"end\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-42dc7261b2a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: range() missing 1 required positional arguments: \"end\""
     ]
    }
   ],
   "source": [
    "torch.range(1,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 搭建一个简易神经网络\n",
    "　　下面通过一个实例来看看如何使用已经掌握的知识，搭建出一个基于PyTorch架构的简易神经网络模型。\n",
    "\n",
    "　　搭建神经网络模型的具体代码如下，这里讲完整的代码分为几部分进行详细介绍，以便大家了解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 导入包\n",
    "　　代码的开始处是相关包的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "batch_n = 100\n",
    "hidden_layer = 100\n",
    "input_data = 1000\n",
    "output_data = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 我们先通过import torch 导入必要的包，然后定义4个整型变量，其中：batch_n是在一个批次中输入数据的数量，值是100，这意味着我们在一个批次中输入100个数据，同时，每个数据包含的数据特征有input_data个，因为input_data的值是1000，所以每个数据的特征就是1000个，hidden_layer用于定义经过隐藏层后保留的数据特征的个数，这里有100个，因为我们的模型只考虑一层隐藏层，所以在代码中仅仅定义了一个隐藏层的参数；output_data是输出的数据，值是10，我们可以将输出的数据看作一个分类结果值得数量，个数10表示我们最后要得到10个分类结果值。\n",
    "\n",
    "　　一个批次的数据从输入到输出的完整过程是：先输入100个具有1000个特征的数据，经过隐藏层后变成100个具有100个特征的数据，再经过输出层后输出100个具有10个分类结果值的数据，在得到输出结果之后计算损失并进行后向传播，这样一次模型的训练就完成了，然后训练这个流程就可以完成指定次数的训练，并达到优化模型参数的目的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 初始化权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(batch_n,input_data)\n",
    "y = torch.randn(batch_n,output_data)\n",
    "w1 = torch.randn(input_data,hidden_layer)\n",
    "w2 = torch.randn(hidden_layer,output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 定义训练次数和学习效率\n",
    "　　在定义好输入，输出和权重参数值之后，就可以开始训练模型和优化权重参数了，在此之前，我们还需要明确训练的总次数和学习效率，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_n = 20\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 由于接下来会使用梯度下降的方法来优化神经网络的参数，所以必须定义后向传播的次数和梯度下降使用的学习效率。在以上代码中使用了epoch_n定义训练的次数，epoch_n的值为20，所以我们需要通过循环的方式让程序进行20次训练，来完成对初始化权重参数的优化和调整。在优化的过程中使用的学习效率learning_rate的值为1e-6，表示0.000001，接下来对模型进行正式训练并对参数进行优化。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 梯度下降优化神经网络的参数\n",
    "　　下面代码通过最外层的一个大循环来保证我们的模型可以进行20次训练，循环内的是神经网络模型具体的前向传播和后向传播代码。参数的优化和更新使用梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0,Loss:52810584.0000\n",
      "Epoch:1,Loss:130577736.0000\n",
      "Epoch:2,Loss:544918912.0000\n",
      "Epoch:3,Loss:879479424.0000\n",
      "Epoch:4,Loss:18022342.0000\n",
      "Epoch:5,Loss:10927354.0000\n",
      "Epoch:6,Loss:7374216.0000\n",
      "Epoch:7,Loss:5301299.0000\n",
      "Epoch:8,Loss:3981101.2500\n",
      "Epoch:9,Loss:3090437.0000\n",
      "Epoch:10,Loss:2464800.0000\n",
      "Epoch:11,Loss:2012175.1250\n",
      "Epoch:12,Loss:1676580.2500\n",
      "Epoch:13,Loss:1422439.2500\n",
      "Epoch:14,Loss:1226006.3750\n",
      "Epoch:15,Loss:1072088.2500\n",
      "Epoch:16,Loss:949370.7500\n",
      "Epoch:17,Loss:849994.3125\n",
      "Epoch:18,Loss:768241.3750\n",
      "Epoch:19,Loss:700308.8125\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_n):\n",
    "    h1 = x.mm(w1)\n",
    "    h1 = h1.clamp(min = 0)\n",
    "    y_pred = h1.mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(\"Epoch:{},Loss:{:.4f}\".format(epoch,loss))\n",
    "    \n",
    "    gray_y_pred = 2*(y_pred - y)\n",
    "    gray_w2 = h1.t().mm(gray_y_pred)\n",
    "    \n",
    "    grad_h = gray_y_pred = gray_y_pred.clone()\n",
    "    grad_h = grad_h.mm(w2.t())\n",
    "    grad_h.clamp(min=0)\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate*gray_w2\n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二：自动梯度\n",
    "　　我们在上面基于PyTorch深度学习框架搭建了一个简易神经网络模型，并通过在代码中使用前向传播和后向传播实现了对这个模型的训练和对权重参数的额优化，不过该模型在结构上很简单，而且神经网络的代码也不复杂。我们在实践中搭建的网络模型都是层次更深的神经网络模型，即深度神经网络模型，结构会有所变化，代码也会更复杂。对于深度的神经网络模型的前向传播使用简单的代码就能实现，但是很难实现涉及该模型中后向传播梯度计算部分的代码，其中最苦难的就是对模型计算逻辑的梳理。\n",
    "\n",
    "　　在PyTorch中提供了一种非常方便的方法，可以帮助我们实现对模型中后向传播梯度的自动计算，避免了“重复造轮子”，这就是接下来要学习的torch.autograd包，通过torch.autograd包，可以使模型参数自动计算在优化过程中需要用到的梯度值，在很大程度上帮助降低了实现后向传播代码的复杂度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 torch.autograd和Variable\n",
    "　　torch.autograd包的主要功能是完成神经网络后向传播中的链式求导，手动实现链式求导的代码会给我们造成很大的困扰，而torch.autograd包中丰富的类减少了这些不必要的麻烦。\n",
    "\n",
    "实现自动梯度功能的过程大概分为以下几步：\n",
    "\n",
    "1 通过输入的Tensor数据类型的变量在神经网络的前向传播过程中生成一张计算图\n",
    "\n",
    "2 根据这个计算图和输出结果准确计算出每个参数需要更新的梯度\n",
    "\n",
    "3 通过完成后向传播完成对参数梯度的更新\n",
    "\n",
    "　　在实践中完成自动梯度需要用到torch.autograd包中的Variable类对我们定义的Tensor数据类型变量进行封装，在封装后，计算图中的各个节点就是一个variable 对象，这样才能应用自动梯度的功能。autograd package是PyTorch中所有神经网络的核心。先了解一些基本知识，然后开始训练第一个神经网络。autograd package提供了Tensors上所有运算的自动求导功能。它是一个按运行定义（define-by-run）的框架，这意味着反向传播是依据代码运行情况而定义的，并且每一个单次迭代都可能不相同。\n",
    "\n",
    "　　autograd.Variable 是这个package的中心类。它打包了一个Tensor，并且支持几乎所有运算。一旦你完成了你的计算，可以调用.backward()，所有梯度就可以自动计算。 　　你可以使用.data属性来访问原始tensor。相对于变量的梯度值可以被积累到.grad中。 　　这里还有一个类对于自动梯度的执行是很重要的：Function（函数） 　　变量和函数是相互关联的，并且建立一个非循环图。每一个变量有一个.grad_fn属性，它可以引用一个创建了变量的函数（除了那些用户创建的变量——他们的grad_fn是空的）。 　　如果想要计算导数，可以调用Variable上的.backward()。如果变量是标量（只有一个元素），你不需要为backward()确定任何参数。但是，如果它有多个元素，你需要确定grad_output参数（这是一个具有匹配形状的tensor）。\n",
    "\n",
    "　　如果已经按照如上的方式完成了相关操作，则在选中了计算图中的某个节点时，这个节点必定是一个Variable对象，用X表示我们选中的节点，那么X.data代表Tensor数据类型 的变量，X.grad也是一个Variable对象，不过他代表的是X的梯度，在想访问梯度值的时候需要X.grad.data\n",
    "\n",
    "　　下面通过一个自动剃度的实例来看看如何使用torch.autograd.Variable类和torch.autograd包，我们同样搭建一个二层结构的神经网络模型，这有利于我们之前搭建的简易神经网络模型的训练和优化过程进行对比，重新实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 导入包 　　代码的开始处是相关包的导入，但是在代码中增加一行，from torch.autograd import Variable，之前定义的不变："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.autograd import Variable\n",
    "batch_n = 100\n",
    "hidden_layer = 100\n",
    "input_data = 1000\n",
    "output_data = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 初始化权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(batch_n , input_data) , requires_grad = False)\n",
    "y = Variable(torch.randn(batch_n , output_data) , requires_grad = False)\n",
    " \n",
    "w1 = Variable(torch.randn(input_data,hidden_layer),requires_grad = True)\n",
    "w2 = Variable(torch.randn(hidden_layer,output_data),requires_grad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### “Variable(torch.randn(batch_n, input_data), requires_grad = False)”这段代码就是之前讲到的用 Variable类对 Tensor数据类型变量进行封装的操作。在以上代码中还使用了一个requires_grad参数，这个参数的赋值类型是布尔型，如果requires_grad的值是False，那么表示该变量在进行自动梯度计算的过程中不会保留梯度值。我们将输入的数据x和输出的数据y的requires_grad参数均设置为False，这是因为这两个变量并不是我们的模型需要优化的参数，而两个权重w1和w2的requires_grad参数的值为True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 定义训练次数和学习效率\n",
    "　　在定义好输入，输出和权重参数值之后，就可以开始训练模型和优化权重参数了，在此之前，我们还需要明确训练的总次数和学习效率，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_n = 20\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 新的模型训练和参数优化\n",
    "　　下面代码通过最外层的一个大循环来保证我们的模型可以进行20次训练，循环内的是神经网络模型具体的前向传播和后向传播代码。参数的优化和更新使用梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 , Loss:242598.2656\n",
      "Epoch:1 , Loss:234487.5000\n",
      "Epoch:2 , Loss:226807.6094\n",
      "Epoch:3 , Loss:219554.5469\n",
      "Epoch:4 , Loss:212698.2812\n",
      "Epoch:5 , Loss:206195.8594\n",
      "Epoch:6 , Loss:200021.3281\n",
      "Epoch:7 , Loss:194156.2344\n",
      "Epoch:8 , Loss:188573.2656\n",
      "Epoch:9 , Loss:183259.8438\n",
      "Epoch:10 , Loss:178192.2031\n",
      "Epoch:11 , Loss:173351.6562\n",
      "Epoch:12 , Loss:168727.4531\n",
      "Epoch:13 , Loss:164307.7969\n",
      "Epoch:14 , Loss:160079.9062\n",
      "Epoch:15 , Loss:156030.4688\n",
      "Epoch:16 , Loss:152148.2812\n",
      "Epoch:17 , Loss:148423.3906\n",
      "Epoch:18 , Loss:144848.4219\n",
      "Epoch:19 , Loss:141427.2812\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_n):\n",
    "    y_pred = x.mm(w1).clamp(min= 0 ).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(\"Epoch:{} , Loss:{:.4f}\".format(epoch, loss.data))\n",
    " \n",
    "    loss.backward()\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    " \n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()# w1.grad  是一个variable对象，代表的是w1的梯度\n",
    "    #w1.grad.data是w1的梯度值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和之前的代码相比，当前的代码更简洁了，之前代码中的后向传播计算部分变成了新代码中的 loss.backward()，这个函数的功能在于让模型根据计算图自动计算每个节点的梯度值并根据需求进行保留，有了这一步，我们的权重参数 w1.data和 w2.data就可以直接使用在自动梯度过程中求得的梯度值w1.data.grad和w2.data.grad，并结合学习速率来对现有的参数进行更新、优化了。在代码的最后还要将本次计算得到的各个参数节点的梯度值通过grad.data.zero_()全部置零，如果不置零，则计算的梯度值会被一直累加，这样就会影响到后续的计算。同样，在整个模型的训练和优化过程中，每个循环都加入了打印loss值的操作，所以最后会得到20个loss值的打印输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "     \n",
    "    def forward(self,input,w1,w2):\n",
    "        x = torch.mm(input,w1)\n",
    "        x = torch.clamp(x,min=0)\n",
    "        x = torch.mm(x,w2)\n",
    "        return x\n",
    "     \n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = Model()\n",
    "这一系列操作相当于完成了对简易神经网络的搭建，然后就只剩下对模型进行训练和对参数进行优化的部分了，代码如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 , Loss:49263152.0000\n",
      "Epoch:1 , Loss:105409384.0000\n",
      "Epoch:2 , Loss:407930048.0000\n",
      "Epoch:3 , Loss:784300992.0000\n",
      "Epoch:4 , Loss:48901044.0000\n",
      "Epoch:5 , Loss:19324732.0000\n",
      "Epoch:6 , Loss:11318168.0000\n",
      "Epoch:7 , Loss:7552420.5000\n",
      "Epoch:8 , Loss:5418730.5000\n",
      "Epoch:9 , Loss:4073183.5000\n",
      "Epoch:10 , Loss:3168096.0000\n",
      "Epoch:11 , Loss:2530418.7500\n",
      "Epoch:12 , Loss:2065817.7500\n",
      "Epoch:13 , Loss:1718402.0000\n",
      "Epoch:14 , Loss:1452981.6250\n",
      "Epoch:15 , Loss:1246797.8750\n",
      "Epoch:16 , Loss:1083958.1250\n",
      "Epoch:17 , Loss:953417.4375\n",
      "Epoch:18 , Loss:847485.5625\n",
      "Epoch:19 , Loss:760560.5625\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    " \n",
    "# 批量输入的数据量\n",
    "batch_n = 100\n",
    "# 通过隐藏层后输出的特征数\n",
    "hidden_layer = 100\n",
    "# 输入数据的特征个数\n",
    "input_data = 1000\n",
    "# 最后输出的分类结果数\n",
    "output_data = 10\n",
    " \n",
    "x = Variable(torch.randn(batch_n , input_data) , requires_grad = False)\n",
    "y = Variable(torch.randn(batch_n , output_data) , requires_grad = False)\n",
    " \n",
    "w1 = Variable(torch.randn(input_data,hidden_layer),requires_grad = True)\n",
    "w2 = Variable(torch.randn(hidden_layer,output_data),requires_grad = True)\n",
    " \n",
    "# 训练次数设置为20\n",
    "epoch_n = 20\n",
    "# 将学习效率设置为0.000001\n",
    "learning_rate = 1e-6\n",
    " \n",
    "for epoch in range(epoch_n):\n",
    " \n",
    "    y_pred = x.mm(w1).clamp(min= 0 ).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(\"Epoch:{} , Loss:{:.4f}\".format(epoch, loss.data))\n",
    " \n",
    "    loss.backward()\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    " \n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
